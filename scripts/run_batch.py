'''
run_batch.py

Authors: Benjamin Wu & Tomer Atzili
Date: 12/13/2023

This program serves to take a batch file (generated by generate_batches.py) and a prompt text file and run the batch and prompt through the GPT4-V(ision) API.
It also takes in an output file path to save the answer in and a debug directory path to save any un-parseable responses. The program attempts to parse the
GPT response into a JSON. Our default prompt includes instructions to the LLM to return its answer in a JSON format, but because of LLM unpredectibility,
some responses cant be parsed. These are send to the debug directory. Each request to the API includes 6 images as to maximize productivity under the
request limit but not exceed the token limit.
'''

from openai import OpenAI
from multiprocessing import Pool
from functools import reduce
from tqdm import tqdm
import base64
import json
import os
import argparse

def parse_arguments():
    '''Parse arguments as dictated by the argument parser.'''
    parser = argparse.ArgumentParser()
    parser.add_argument('--batch_fpath', '-b', required=True, help='Path to single batch file from those generated by generate_batches.')
    parser.add_argument('--prompt_fpath', '-p', required=True, help='Path to text file with the prompt in it. Our prompt is at prompts/current.txt')
    parser.add_argument('--output_fpath', '-o', required=True, help='File to put output in. Should be "batchWhateverYouWant.json".')
    parser.add_argument('--debug_dir', '-d', help='Directory to store any API responses that couldn\'t properly be parsed.')
    args = parser.parse_args()
    return args

def setup(batch_fpath, prompt_fpath):
    '''Sets up important variables for use in other functions. Loads in the image batches and prompt and starts the API client.'''
    # load in image batches
    with open(batch_fpath, 'r') as fp:
        image_batches = json.load(fp)

    # load in prompt
    with open(prompt_fpath, 'r') as fp:
        prompt = fp.read()
    
    # Start API client
    client = OpenAI()
    return (image_batches, prompt, client)

def encode_image(image_path):
    '''encodes an image into base64'''
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

def get_gpt_response(image_path_list, client, prompt):
    '''sends the prompt and the list of images to gpt. returns the text response.'''
    response = client.chat.completions.create( #This is the actual command to contact API (your environment must be set up with API key to use)
        model="gpt-4-vision-preview", #Could change as API changes from preview
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "text", 
                        "text": prompt
                    },
                ] + [
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{encode_image(image_path)}",
                            "detail": "low",
                        }
                    } for image_path in image_path_list
                ],
            }
        ],
        max_tokens=600*len(image_path_list),
    )

    return response.choices[0].message.content





def run_image_batch(batch_id, client, prompt, image_batches, output_fpath, debug_path, batch_fpath):
    '''
    Runs a batch through GPT. It attempts to parse the response into a json, but due to LLM variability, the response isn't always parseable.
    In this case, the response is outputted as is in the debug_path.
    Returns a dictionary of image_path -> q/a pairs for a single batch (currently 6 images).
    '''
    batch = image_batches[batch_id]

    # run batch through gpt
    response_txt = get_gpt_response(batch, client, prompt)

    debug_dir = debug_path
    if debug_dir is None:
        debug_dir = os.path.dirname(output_fpath)

    try:
        #JSON Parsing
        image_qa_list = json.loads(response_txt[response_txt.index('```json')+len('```json'):response_txt.rindex('```')])
    except Exception as e:
        # if parsing fails, output the response text for debugging
        # save gpt response to file w/ batch_id
        answer_name, answer_ext = os.path.splitext(os.path.basename(output_fpath))
        os.makedirs(debug_dir, exist_ok=True)

        debug_fpath = os.path.join(debug_dir, f'debug_batch{batch_id}_{answer_name}{answer_ext}')
        with open(debug_fpath, 'w') as fp:
            json.dump({
                'batch_fpath': batch_fpath,
                'batch_id': batch_id,
                'error': str(e),
                'response': response_txt,
            }, fp, indent=4)
        print(f"Failed to parse GPT JSON output. Wrote response to {debug_fpath}.")
        raise e
    # create image path->qa list dict
    ipath2qa_pairs = dict()
    for i in range(len(image_qa_list)):
        image_path = batch[i]
        image_qa_pairs = image_qa_list[i]
        ipath2qa_pairs[image_path] = image_qa_pairs
    return ipath2qa_pairs

def run_batch_safe(batch_id, client, prompt, image_batches, output_fpath, debug_path, batch_fpath):
    '''Utility function to safely run a batch. Will always return a dictionary.'''
    try:
        return run_image_batch(batch_id, client, prompt, image_batches, output_fpath, debug_path, batch_fpath)
    except Exception as e:
        print(e)
        return dict()

# multiprocessing disabled because of tokens per minute rate limit (10,000). in the future, consider exponential backoff
# with tqdm(total=len(image_batches), miniters=1) as loading_bar:
#     with Pool() as p:
#         data_dicts = []
#         for result in p.imap_unordered(run_batch_safe, range(len(image_batches))):
#             data_dicts.append(result)
#             loading_bar.update()

def run(image_batches, client, prompt, output_fpath, debug_path, batch_fpath):
    '''Function to run the batch-running functions on each batch in image_batches and then combine them into a large dictionary.
    Returns a dictionary with each image path and their Q/A pairs.'''
    data_dicts = [run_batch_safe(i, client, prompt, image_batches, output_fpath, debug_path, batch_fpath) for i in tqdm(range(len(image_batches)))]
    ipath2qa_pairs = reduce(lambda a, b: a | b, data_dicts)
    return ipath2qa_pairs

def save_batch(prompt, ipath2qa_pairs, output_fpath):
    '''Saves the data from the run function, along with the prompt, to a json specified by the output_fpath argument.'''
    os.makedirs(os.path.dirname(output_fpath), exist_ok=True)
    with open(output_fpath, 'w') as fp:
        fp.write(json.dumps({
            'prompt': prompt,
            'data': ipath2qa_pairs
        }, indent=4))

if __name__ == '__main__':
    args = parse_arguments()
    image_batches, prompt, client = setup(args.batch_fpath, args.prompt_fpath)
    ipath2qa_pairs = run(image_batches, client, prompt, args.output_fpath, args.debug_dir, args.batch_fpath)
    save_batch(prompt, ipath2qa_pairs, args.output_fpath)